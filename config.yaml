# JARVIS Configuration
# ====================
# Hybrid LLM: DictaLM for Planner/Reflector/Personality (short 4k ctx), Qwen3 for tools/self-evolution
# Latency target: sub-2s end-to-end on RTX 4080 + 16GB RAM

# Hybrid LLM (per-node binding in LangGraph)
llm:
  # DictaLM: Planner + Reflector + Personality ONLY (short 4k context)
  conversation_model: "aminadaven/dictalm2.0-instruct:q5_K_M"
  # Qwen3: ALL tool calls, self-evolution, JSON (use vLLM/exllama2 for 2x speed)
  tool_model: "qwen3:4b"
  host: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 256  # Reflector output limit (keeps responses brief, faster)
  context_window: 6  # Max turns (reduced for sub-2s latency)
  planner_temperature: 0.5
  planner_max_tokens: 512  # Shorter = faster
  reflector_temperature: 0.5
  # Context limits: DictaLM 8k max, Qwen 4k (Ollama num_ctx)
  num_ctx_planner: 8192
  num_ctx_reflector: 4096
  num_ctx_tool: 4096

# Voice (optimized for sub-800ms first token)
voice:
  preferred_language: "en"
  stt_model: "large-v3-turbo"
  stt_device: "cuda"
  stt_language: "en"
  stt_beam_size: 3  # 3 = faster (was 5), slight quality trade-off
  stt_compute_type: "int8"  # int8 = faster on CPU/GPU
  tts_engine: "piper"
  tts_voice: "jgkawell/jarvis"
  tts_quality: "medium"
  hebrew_voice: "he-IL-AvriNeural"
  force_hebrew_tts: false  # Only when needed (Hebrew text) - edge-tts is slower
  preload_tts: true  # Preload Piper at startup - "As you wish, Sir..." within 800ms
  stream_tts: true   # Stream first token -> immediate TTS (sub-800ms first audio)
  tts_speed: 1.0  # Regular speed
  max_response_words: 50  # Keep responses brief for voice
  min_rms: 0.0010  # Don't respond if recording is quieter. 0.002 was too strict for some mics.
  recorder_sample_rate: 16000
  recorder_silence_duration: 2.5   # Seconds of silence before considering speech done (longer = more patient for pauses/thinking)
  recorder_silence_threshold: 0.012 # RMS below this = silence (slightly higher helps with non-native speaker pauses)
  push_to_talk_seconds: 5   # Fixed recording length for push-to-talk
  follow_up_seconds: 8     # After response, record this long for follow-up (no "Hey Jarvis" needed)
  # False positive reduction (Issue 1)
  min_audio_length: 1.0    # Ignore recordings shorter than this (seconds) - filters noise bursts
  min_transcript_words: 3  # Discard transcriptions with fewer words (likely noise artifacts)
  use_vad: true            # Voice activity detection in STT - filters silence/background
  # Continuous conversation (Issue 2)
  silence_timeout: 15      # Seconds of silence to end active conversation (10-20 typical)
  listening_prompt: "Listening, Sir."  # Spoken after response to indicate ready for follow-up

# Wake words (openwakeword - 100% local)
# "Hey Jarvis" (English) + "היי ג'ארוויס" (Hebrew - requires custom training)
wake_word:
  models: ["hey_jarvis_v0.1"]
  threshold: 0.15  # Legacy alias - use wake_confidence if set
  wake_confidence: 0.15  # 0.7-0.85 reduces false positives from background noise (higher = stricter)
  device: null     # Set to mic ID from: python -c "import sounddevice; print(sounddevice.query_devices())"
  cooldown_seconds: 3  # Ignore rapid re-triggers after wake
  noise_gate_rms: 0.005  # Skip wake detection if chunk RMS below this (reduces false triggers)

# Memory (Chroma queries target <100ms)
memory:
  db_path: "data/jarvis.db"
  chroma_path: "data/chroma"
  embedding_model: "nomic-embed-text"
  max_memories: 3  # Fewer = faster queries
  chroma_cache_recent: true  # Cache recent facts for <100ms

# DDGS web search - use fixed Chrome (not random) for consistent browser-like requests
ddgs:
  impersonate: "chrome_145"   # Chrome 145 - primp 1.0.0 supported, avoids "using random" warning
  impersonate_os: "windows"   # Match your OS for realistic fingerprint

# Tools
tools:
  allowed_directories:
    - "~"
  max_search_results: 5
  command_timeout: 30
  # Time verification: "chrome" = use your Chrome via time.is (most reliable), "api" = time.now API only
  time_verify: "chrome"

# Heartbeat
heartbeat:
  interval_minutes: 30

# General
debug: false
log_level: "INFO"
# Debug timing: "Planner: 420ms, Tools: 180ms, TTS: 320ms"
timing: false  # Set true to see per-node latency
show_latency: true  # Display [2.3s] after each response

# Ollama (run with): ollama serve --flash-attn --gpu-layers -1
# Or set env: OLLAMA_FLASH_ATTENTION=1 OLLAMA_NUM_GPU=1
# Per-model num_ctx: DictaLM 8192, Qwen 4096 (set in Modelfile or via API)
