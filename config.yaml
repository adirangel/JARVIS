# JARVIS Configuration
# ====================
# Hybrid LLM: DictaLM for Planner/Reflector/Personality (short 4k ctx), Qwen3 for tools/self-evolution
# Latency target: sub-2s end-to-end on RTX 4080 + 16GB RAM

# Hybrid LLM (per-node binding in LangGraph)
llm:
  # DictaLM: Planner + Reflector + Personality ONLY (short 4k context)
  conversation_model: "aminadaven/dictalm2.0-instruct:q5_K_M"
  # Qwen3: ALL tool calls, self-evolution, JSON (use vLLM/exllama2 for 2x speed)
  tool_model: "qwen3:4b"
  host: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 256  # Reflector output limit (keeps responses brief, faster)
  context_window: 6  # Max turns (reduced for sub-2s latency)
  planner_temperature: 0.5
  planner_max_tokens: 512  # Shorter = faster
  reflector_temperature: 0.5
  # Context limits: DictaLM 8k max, Qwen 4k (Ollama num_ctx)
  num_ctx_planner: 8192
  num_ctx_reflector: 4096
  num_ctx_tool: 4096

# Voice (optimized for sub-800ms first token)
voice:
  preferred_language: "en"
  stt_model: "large-v3-turbo"
  stt_device: "cuda"
  stt_language: "en"
  stt_beam_size: 3  # 3 = faster (was 5), slight quality trade-off
  stt_compute_type: "int8"  # int8 = faster on CPU/GPU
  tts_engine: "piper"
  tts_voice: "jgkawell/jarvis"
  tts_quality: "medium"
  hebrew_voice: "he-IL-AvriNeural"
  force_hebrew_tts: false  # Only when needed (Hebrew text) - edge-tts is slower
  preload_tts: true  # Preload Piper at startup - "As you wish, Sir..." within 800ms
  stream_tts: true   # Stream first token -> immediate TTS (sub-800ms first audio)
  tts_speed: 1.0  # Regular speed
  max_response_words: 50  # Keep responses brief for voice
  min_rms: 0.0010  # Don't respond if recording is quieter. 0.002 was too strict for some mics.
  recorder_sample_rate: 16000
  recorder_silence_duration: 2.5   # Seconds of silence before considering speech done (longer = more patient for pauses/thinking)
  recorder_silence_threshold: 0.012 # RMS below this = silence (slightly higher helps with non-native speaker pauses)
  push_to_talk_seconds: 5   # Fixed recording length for push-to-talk
  follow_up_seconds: 8     # After response, record this long for follow-up (no "Hey Jarvis" needed)

# Wake words (openwakeword - 100% local)
# "Hey Jarvis" (English) + "היי ג'ארוויס" (Hebrew - requires custom training)
wake_word:
  models: ["hey_jarvis_v0.1"]
  threshold: 0.15  # Lower = more sensitive. 0.15-0.35 typical. 0.0021 too low (model scores 0-1)
  device: null     # Set to mic ID from: python -c "import sounddevice; print(sounddevice.query_devices())"
  cooldown_seconds: 3  # Ignore rapid re-triggers after wake

# Memory (Chroma queries target <100ms)
memory:
  db_path: "data/jarvis.db"
  chroma_path: "data/chroma"
  embedding_model: "nomic-embed-text"
  max_memories: 3  # Fewer = faster queries
  chroma_cache_recent: true  # Cache recent facts for <100ms

# Tools
tools:
  allowed_directories:
    - "~"
  max_search_results: 5
  command_timeout: 30

# Heartbeat
heartbeat:
  interval_minutes: 30

# General
debug: false
log_level: "INFO"
# Debug timing: "Planner: 420ms, Tools: 180ms, TTS: 320ms"
timing: false  # Set true to see per-node latency
show_latency: true  # Display [2.3s] after each response

# Ollama (run with): ollama serve --flash-attn --gpu-layers -1
# Or set env: OLLAMA_FLASH_ATTENTION=1 OLLAMA_NUM_GPU=1
# Per-model num_ctx: DictaLM 8192, Qwen 4096 (set in Modelfile or via API)
